{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e429df",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Running HDBSCAN on partial distance matrices?\n",
    "\n",
    "What can we get from HDBSCAN on a sparse distance matrix having non-zero entries only between high dimensional k-NN data pairs? \n",
    "* First we cheat: the distance values are the low dimensional distances (after dimension reduction with UMAP). If we use a small number of NNs (same as for the UMAP projection say), running HDBSCAN on the sparse matrix does not provide good clusters. The larger the number of NNs - the larger the number of non-zero values - the closer we get to HDBSCAN result on UMAP reduction. I am still surprised that by *cheating* a lot and providing a kNN graph (with large k value) of the low dim distances, we do not get the same results as HDBSCAN on the low dim points. This brings us to the next natural question: how do NNs correspond between high and low dimensions?\n",
    "* from UMAP weighted adjacency matrix with unknown values encoded as infinite values. - I actually don't know how to turn the UMAP weight into a reasonable distance. I just used the inverse. (doesn't work!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4875757-c802-423c-8bf8-8efd45ff78e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* \u001b[32mmain\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!git branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "52d3bdf4-9b88-4008-93e6-5e6f656e5b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pendigits', 'coil', 'mnist', 'usps', 'buildings']\n"
     ]
    }
   ],
   "source": [
    "execfile('functions/data_specifics.py')\n",
    "execfile('functions/graph_functions.py')\n",
    "print(data_set_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84a03c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, silhouette_score\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import hdbscan\n",
    "import umap\n",
    "from sklearn.neighbors import KNeighborsTransformer\n",
    "import pynndescent\n",
    "\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import collections\n",
    "import igraph as ig\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3703c8ac-e577-4f60-b1ac-5a37556e4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_graph_edge_properties(G, vertex_high_representation=None, vertex_low_representation=None):\n",
    "    \n",
    "    G.es['umap_weight'] = G.es['weight']\n",
    "    \n",
    "    simplices = G.cliques(min=3, max=3)\n",
    "    edge_in_simplex = []\n",
    "    for x in simplices:\n",
    "        edge_in_simplex += G.get_eids([pair for pair in itertools.combinations(x, r=2)], directed=False)\n",
    "    G.es[\"nb_triangles\"] = 0\n",
    "    for k, v in collections.Counter(edge_in_simplex).items():\n",
    "        G.es[k][\"nb_triangles\"] = v\n",
    "    \n",
    "    if(vertex_low_representation is not None):\n",
    "        G.es['lowdim_dist'] = [euclidean( vertex_low_representation[e.source], \n",
    "                                         vertex_low_representation[e.target] ) \n",
    "                               for e in G.es]\n",
    "        \n",
    "    if(vertex_high_representation is not None):\n",
    "        G.es['highdist_dist'] = [euclidean( vertex_high_representation[e.source], \n",
    "                                         vertex_high_representation[e.target] ) \n",
    "                               for e in G.es] \n",
    "        \n",
    "    for v in G.vs:\n",
    "        x = {e:G.es[e]['highdist_dist'] for e in G.incident(v)}\n",
    "        for i,e in enumerate(sorted(x, key=x.get)):\n",
    "            G.es[e]['highdist_rank'] = i\n",
    "            \n",
    "    for v in G.vs:\n",
    "        x = {e:G.es[e]['lowdim_dist'] for e in G.incident(v)}\n",
    "        for i,e in enumerate(sorted(x, key=x.get)):\n",
    "            G.es[e]['lowdist_rank'] = i\n",
    "    \n",
    "    for v in G.vs:\n",
    "        x = {e:G.es[e]['umap_weight'] for e in G.incident(v)}\n",
    "        for i,e in enumerate(sorted(x, key=x.get)):\n",
    "            G.es[e]['umap_rank'] = i\n",
    "    return(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01ef2ebc-c789-4f17-b729-c3d2eac57439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_edge_pruning_triangle(G, min_triangle=0):\n",
    "    G_pendigits_filter = G_pendigits.copy()\n",
    "    edges_rm = [(u,v) for u,v,e in G_pendigits_filter.edges(data=True) if e['nb_triangles']<min_triangle]\n",
    "    G_pendigits_filter.remove_edges_from(edges_rm)\n",
    "    \n",
    "    cc_labels = [-1]*G_pendigits.number_of_nodes()\n",
    "    cc_iter = list(nx.connected_components(G_pendigits_filter))\n",
    "    for i, c in enumerate(cc_iter):\n",
    "        if(len(c)<2):\n",
    "            continue\n",
    "        for x in c:\n",
    "            cc_labels[x] = i\n",
    "    return(cc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8ab37a16-629f-4fd8-91c9-ac31315c42eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdbscan_on_cc(A, min_samples, min_cluster_size):\n",
    "    A_cc = scipy.sparse.csgraph.connected_components(A, return_labels=True)\n",
    "    final_clusters = np.array([-1]*A.shape[0])\n",
    "    hd_umap_labels_cc = dict()\n",
    "    d0 = {-1:-1}\n",
    "    for i in range(A_cc[0]):\n",
    "        w = (A_cc[1]==i)\n",
    "        n_points = sum(w)\n",
    "        print(n_points)\n",
    "        if(n_points>min_cluster_size):\n",
    "            hd_umap_labels_cc[i] = hdbscan.HDBSCAN(min_samples=min_samples, min_cluster_size=min_cluster_size, metric='precomputed', allow_single_cluster=True, max_dist=2*A.max()).fit_predict(A[w, :][:, w])\n",
    "            m = max(hd_umap_labels_cc[i])\n",
    "            M = max(final_clusters)\n",
    "            d1 = {i:M+i+1 for i in range(m+1)}\n",
    "            new_cluster_id = {**d0, **d1} \n",
    "            print(new_cluster_id)\n",
    "            final_clusters[w] = [new_cluster_id[c] for c in hd_umap_labels_cc[i]]\n",
    "    return(final_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a62c3118-989e-4235-8910-0ae188eb583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdbscan_connect(A, min_samples, min_cluster_size):\n",
    "    A_cc = scipy.sparse.csgraph.connected_components(A, return_labels=True)\n",
    "    first_occ = []\n",
    "    m = max(A.data)\n",
    "    for i in range(A_cc[0]):\n",
    "        first_occ.append(np.where(A_cc[1] == i)[0][0])\n",
    "        if(i>0):\n",
    "            A[first_occ[i-1], first_occ[i]] = A[first_occ[i], first_occ[i-1]] = 2*m\n",
    "    final_clusters = hdbscan.HDBSCAN(min_samples=min_samples, min_cluster_size=min_cluster_size, metric='precomputed', max_dist=2*A.max()).fit_predict(A)\n",
    "    return(final_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4359f5ee-37bf-490b-8402-a2305cbdd1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_umap = 15\n",
    "knn_graph = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354bd894",
   "metadata": {},
   "source": [
    "# Pendigits clustering scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9e947e01-c98c-4e30-9caa-b78fcf078621",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 0\n",
    "raw_data, targets, dataset_name = get_dataset(dataset_id)\n",
    "\n",
    "G, A, dists = get_umap_graph(raw_data, dataset_id=dataset_id, set_op_mix_ratio=1.0, return_all=True)\n",
    "umap_rep = get_umap_vectors(dataset_id=dataset_id, raw_data=raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2c44a614-58da-42ce-aa19-c5f320b3b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = enrich_graph_edge_properties(G, \n",
    "                             vertex_high_representation=raw_data, \n",
    "                             vertex_low_representation=umap_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cfa065ad-1a75-42c6-9e88-e20f41504301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "igraph.Edge(<igraph.Graph object at 0x7fd176781750>, 0, {'weight': 0.16689156, 'umap_weight': 0.16689156, 'nb_triangles': 3, 'lowdim_dist': 0.28059548139572144, 'highdist_dist': 20.784608840942383, 'highdist_rank': 8, 'lowdist_rank': 7, 'umap_rank': 5})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.es[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "36c66182-0fb4-480a-aa90-467765384093",
   "metadata": {},
   "outputs": [],
   "source": [
    "D0 = A.copy()\n",
    "for e in G.es:\n",
    "    D0[e.target, e.source] = D0[e.source, e.target] = e['lowdim_dist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dcc505eb-9655-427d-a6bc-d3ac7fc27f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = get_dataset_params(dataset_id)\n",
    "hd_umap_labels0 = hdbscan.HDBSCAN(min_samples=params['min_samples'], min_cluster_size=params['min_cluster_size'], metric='precomputed').fit_predict(D0)\n",
    "hd_umap_labels = hdbscan.HDBSCAN(min_samples=params['min_samples'], min_cluster_size=params['min_cluster_size']).fit_predict(umap_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f9377b2-008a-4163-aa89-9c206e58b6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of clustering on lower dim data points\n",
      "10 clusters\n",
      "ARI : 0.9185149200427103\n",
      "AMI : 0.9320899303214291\n",
      "Evaluation of clustering on partial distance matrix\n",
      "10 clusters\n",
      "ARI : 0.9163295428694507\n",
      "AMI : 0.9293292186972308\n",
      "Similarity between the two partitions\n",
      "ARI : 0.9961603978065179\n",
      "AMI : 0.9938411946343767\n"
     ]
    }
   ],
   "source": [
    "print('Evaluation of clustering on lower dim data points')\n",
    "print(f'{max(hd_umap_labels)+1} clusters')\n",
    "print(f'ARI : {adjusted_rand_score(targets, hd_umap_labels)}')\n",
    "print(f'AMI : {adjusted_mutual_info_score(targets, hd_umap_labels)}')\n",
    "print('Evaluation of clustering on partial distance matrix')\n",
    "print(f'{max(hd_umap_labels0)+1} clusters')\n",
    "print(f'ARI : {adjusted_rand_score(targets, hd_umap_labels0)}')\n",
    "print(f'AMI : {adjusted_mutual_info_score(targets, hd_umap_labels0)}')\n",
    "print('Similarity between the two partitions')\n",
    "print(f'ARI : {adjusted_rand_score(hd_umap_labels0, hd_umap_labels)}')\n",
    "print(f'AMI : {adjusted_mutual_info_score(hd_umap_labels0, hd_umap_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "079e9720-4bf5-4bed-9deb-279cc8bd3782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdbscan_on_partial_dist(dataset_id):\n",
    "    raw_data, targets, dataset_name = get_dataset(dataset_id)\n",
    "    display(Markdown(f'## {dataset_name}'))\n",
    "\n",
    "    G, A, dists = get_umap_graph(raw_data, dataset_id=dataset_id, set_op_mix_ratio=1.0, return_all=True)\n",
    "    umap_rep = get_umap_vectors(dataset_id=dataset_id, raw_data=raw_data)\n",
    "    \n",
    "    G = enrich_graph_edge_properties(G, \n",
    "                             vertex_high_representation=raw_data, \n",
    "                             vertex_low_representation=umap_rep)\n",
    "    \n",
    "    D0 = A.copy()\n",
    "    for e in G.es:\n",
    "        D0[e.target, e.source] = D0[e.source, e.target] = e['lowdim_dist']\n",
    "    D_cc = scipy.sparse.csgraph.connected_components(D0, return_labels=True)\n",
    "        \n",
    "    params = get_dataset_params(dataset_id)\n",
    "    if(D_cc[0]==1):\n",
    "        hd_umap_labels0 = hdbscan.HDBSCAN(min_samples=params['min_samples'], min_cluster_size=params['min_cluster_size'], metric='precomputed', max_dist=2*D0.max()).fit_predict(D0)\n",
    "    else:\n",
    "        hd_umap_labels0 = hdbscan_connect(D0, min_samples=params['min_samples'], min_cluster_size=params['min_cluster_size'])\n",
    "    hd_umap_labels = hdbscan.HDBSCAN(min_samples=params['min_samples'], min_cluster_size=params['min_cluster_size']).fit_predict(umap_rep)  \n",
    "    \n",
    "    print('Evaluation of clustering on lower dim data points')\n",
    "    print(f'{max(hd_umap_labels)+1} clusters')\n",
    "    print(f'ARI : {adjusted_rand_score(targets, hd_umap_labels)}')\n",
    "    print(f'AMI : {adjusted_mutual_info_score(targets, hd_umap_labels)}')\n",
    "    print('Evaluation of clustering on partial distance matrix')\n",
    "    print(f'{max(hd_umap_labels0)+1} clusters')\n",
    "    print(f'ARI : {adjusted_rand_score(targets, hd_umap_labels0)}')\n",
    "    print(f'AMI : {adjusted_mutual_info_score(targets, hd_umap_labels0)}')\n",
    "    print('Similarity between the two partitions')\n",
    "    print(f'ARI : {adjusted_rand_score(hd_umap_labels0, hd_umap_labels)}')\n",
    "    print(f'AMI : {adjusted_mutual_info_score(hd_umap_labels0, hd_umap_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fcb1f228-9b5f-48cd-aaa6-8d019b52a68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## pendigits"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of clustering on lower dim data points\n",
      "10 clusters\n",
      "ARI : 0.9185149200427103\n",
      "AMI : 0.9320899303214291\n",
      "Evaluation of clustering on partial distance matrix\n",
      "10 clusters\n",
      "ARI : 0.9163295428694507\n",
      "AMI : 0.9293292186972308\n",
      "Similarity between the two partitions\n",
      "ARI : 0.9961603978065179\n",
      "AMI : 0.9938411946343767\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## coil"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/home/vmpouli/.conda/envs/high-dim-easydata/lib/python3.7/site-packages/sklearn/manifold/_spectral_embedding.py:261: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  \"Graph is not fully connected, spectral embedding may not work as expected.\"\n",
      "/disk/home/vmpouli/.conda/envs/high-dim-easydata/lib/python3.7/site-packages/scipy/sparse/_index.py:82: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of clustering on lower dim data points\n",
      "19 clusters\n",
      "ARI : 0.7967647891406653\n",
      "AMI : 0.9462144192674433\n",
      "Evaluation of clustering on partial distance matrix\n",
      "19 clusters\n",
      "ARI : 0.7962790399627984\n",
      "AMI : 0.9454387450892846\n",
      "Similarity between the two partitions\n",
      "ARI : 0.980754849586676\n",
      "AMI : 0.9875228346539583\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## mnist"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of clustering on lower dim data points\n",
      "12 clusters\n",
      "ARI : 0.8987078065717921\n",
      "AMI : 0.8868226537248037\n",
      "Evaluation of clustering on partial distance matrix\n",
      "9 clusters\n",
      "ARI : 0.5023728582734822\n",
      "AMI : 0.722507012231422\n",
      "Similarity between the two partitions\n",
      "ARI : 0.4962328654540124\n",
      "AMI : 0.7178490953092573\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## usps"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of clustering on lower dim data points\n",
      "9 clusters\n",
      "ARI : 0.8824676944734986\n",
      "AMI : 0.9004545663772475\n",
      "Evaluation of clustering on partial distance matrix\n",
      "9 clusters\n",
      "ARI : 0.6128864462599586\n",
      "AMI : 0.7564780387572362\n",
      "Similarity between the two partitions\n",
      "ARI : 0.6549619081041632\n",
      "AMI : 0.7870445119824904\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## buildings"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/home/vmpouli/.conda/envs/high-dim-easydata/lib/python3.7/site-packages/scipy/sparse/_index.py:82: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of clustering on lower dim data points\n",
      "100 clusters\n",
      "ARI : 0.2458851298996959\n",
      "AMI : 0.6307848503576966\n",
      "Evaluation of clustering on partial distance matrix\n",
      "92 clusters\n",
      "ARI : 0.3013433418091169\n",
      "AMI : 0.638557956800602\n",
      "Similarity between the two partitions\n",
      "ARI : 0.6905099857802873\n",
      "AMI : 0.9246471394105523\n"
     ]
    }
   ],
   "source": [
    "# For each data set, run the above analysis\n",
    "for i in range(5):\n",
    "    hdbscan_on_partial_dist(dataset_id=i)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21f8c2b7-d642-4ed5-9e34-4c61aee061e9",
   "metadata": {},
   "source": [
    "def hdbscan_on_***(dataset_id):\n",
    "    raw_data, targets, dataset_name = get_dataset(dataset_id)\n",
    "    display(Markdown(f'## {dataset_name} - UMAP weight'))\n",
    "\n",
    "    G, A, dists = get_umap_graph(raw_data, dataset_id=dataset_id, set_op_mix_ratio=1.0, return_all=True)\n",
    "    umap_rep = get_umap_vectors(dataset_id=dataset_id, raw_data=raw_data)\n",
    "    \n",
    "    G = enrich_graph_edge_properties(G, \n",
    "                             vertex_high_representation=raw_data, \n",
    "                             vertex_low_representation=umap_rep)\n",
    "    \n",
    "    D0 = A.copy()\n",
    "    for e in G.es:\n",
    "        D0[e.target, e.source] = D0[e.source, e.target] = *** PUT WATHEVER HERE ****\n",
    "    D_cc = scipy.sparse.csgraph.connected_components(D0, return_labels=True)\n",
    "        \n",
    "    params = get_dataset_params(dataset_id)\n",
    "    if(D_cc[0]==1):\n",
    "        hd_umap_labels0 = hdbscan.HDBSCAN(min_samples=params['min_samples'], min_cluster_size=params['min_cluster_size'], metric='precomputed', max_dist=2*D0.max()).fit_predict(D0)\n",
    "    else:\n",
    "        hd_umap_labels0 = hdbscan_connect(D0, min_samples=params['min_samples'], min_cluster_size=params['min_cluster_size'])\n",
    "    hd_umap_labels = hdbscan.HDBSCAN(min_samples=params['min_samples'], min_cluster_size=params['min_cluster_size']).fit_predict(umap_rep)  \n",
    "    \n",
    "    print('Evaluation of clustering on lower dim data points')\n",
    "    print(f'{max(hd_umap_labels)+1} clusters')\n",
    "    print(f'ARI : {adjusted_rand_score(targets, hd_umap_labels)}')\n",
    "    print(f'AMI : {adjusted_mutual_info_score(targets, hd_umap_labels)}')\n",
    "    print('Evaluation of clustering on partial distance matrix')\n",
    "    print(f'{max(hd_umap_labels0)+1} clusters')\n",
    "    print(f'ARI : {adjusted_rand_score(targets, hd_umap_labels0)}')\n",
    "    print(f'AMI : {adjusted_mutual_info_score(targets, hd_umap_labels0)}')\n",
    "    print('Similarity between the two partitions')\n",
    "    print(f'ARI : {adjusted_rand_score(hd_umap_labels0, hd_umap_labels)}')\n",
    "    print(f'AMI : {adjusted_mutual_info_score(hd_umap_labels0, hd_umap_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990dd88f-0869-408c-8cd4-84720a0b682c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-high-dim-easydata]",
   "language": "python",
   "name": "conda-env-.conda-high-dim-easydata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
